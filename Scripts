## for replacing ">" in the aceession number with ">speciesname" and "|" with "_" using sed for the ease of identifaction that sequnce belongs to which species.

#!/bin/bash

#SBATCH --partition=defq                                              #using defq for the quick and small jobs
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=50g
#SBATCH --time=10:00:00
#SBATCH --job-name=replace_accession_query sequence                   #change job name
#SBATCH --output=/gpfs01/home/mbxss25/slurm-%x-%j.out                 #replace with your username
#SBATCH --mail-type=ALL
#SBATCH --mail-user=mbxss25@exmail.nottingham.ac.uk                   #replace with your username

# Directory containing your TXT files
input_dir="/mbxss25/data/dom/selected_genes"                  #path of directory where file are stored conating accession numbers
output_dir="/mbxss25/data/dom/query_seqs"                             #path of directory where you wish to store new files.
mkdir -p "$output_dir"                                                # Ensure the output directory exists

# Process each .txt file in the directory
for file in "$input_dir"/*.txt; do
    # Extract the base filename without the extension    
    base_filename=$(basename "$file" .txt)
    
    # Construct the output filename
    output_file="$output_dir/${base_filename}.txt"

    # Process the file and write to the output directory
    sed -e "s/>/>${base_filename}_/" -e 's/|/_/g' "$file" > "$output_file"
    
    echo "Processed and output to $output_file"
done

echo "All files have been processed."

#Input file: 1. selected_genes.zip
#Output directory: 2. query_seqs.zip

#Note: zip files are uploaded only because of github limitation. Original file does not have .zip extention.
------------------------------------------------------------------------------------------------------------------------------------------------------
## for replacing ">" in the aceession number with ">speciesname" and "|" with "_" using sed for the ease of identifaction that sequnce belongs to which species.

#!/bin/bash

#SBATCH --partition=defq                                              #using defq for the quick and small jobs
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=50g
#SBATCH --time=10:00:00
#SBATCH --job-name=replace_accession                                  #change job name
#SBATCH --output=/gpfs01/home/mbxss25/slurm-%x-%j.out                 #replace with your username
#SBATCH --mail-type=ALL
#SBATCH --mail-user=mbxss25@exmail.nottingham.ac.uk                   #replace with your username

# Directory containing your TXT files
input_dir="/mbxss25/data/dom/subject_dborg"                  #path of directory where file are stored conating accession numbers
output_dir="/mbxss25/data/dom/subject_db"                             #path of directory where you wish to store new files.
mkdir -p "$output_dir"                                                # Ensure the output directory exists

# Process each .txt file in the directory
for file in "$input_dir"/*.txt; do
    # Extract the base filename without the extension    
    base_filename=$(basename "$file" .txt)
    
    # Construct the output filename
    output_file="$output_dir/${base_filename}.txt"

    # Process the file and write to the output directory
    sed -e "s/>/>${base_filename}_/" -e 's/|/_/g' "$file" > "$output_file"
    
    echo "Processed and output to $output_file"
done

echo "All files have been processed."

#Input file: 3_subject_dborg.zip
#Output directory: 4_subject_db.zip

#Note: zip files are uploaded only because of github limitation. Original file does not have .zip extention.
------------------------------------------------------------------------------------------------------------------------------------------------------------------
# To translate the DNA sequnces into amino acids, using biopython.

from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
import sys
import os

def translate_dna(input_file, output_file):
    records = SeqIO.parse(input_file, "fasta")
    protein_records = []

    for record in records:
        seq_length = len(record.seq)
        trim_length = seq_length - (seq_length % 3)
        trimmed_seq = record.seq[:trim_length]
        protein_seq = trimmed_seq.translate()
        protein_record = SeqRecord(protein_seq, id=record.id, description="translated sequence")
        protein_records.append(protein_record)

    SeqIO.write(protein_records, output_file, "fasta")

def ensure_directory_exists(output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

def process_files(input_path, output_dir):
    ensure_directory_exists(output_dir)
    
    if os.path.isdir(input_path):
        # Process each file in the directory
        for filename in os.listdir(input_path):
            file_path = os.path.join(input_path, filename)
            if filename.endswith(".txt"):  # Ensure processing only text files
                output_file = os.path.join(output_dir, filename.replace('.txt', '_protein.fasta'))
                translate_dna(file_path, output_file)
                print(f"Translated {file_path} to {output_file}")
    elif os.path.isfile(input_path):
        # Process a single file
        if input_path.endswith(".txt"):  # Ensure processing only text files
            filename = os.path.basename(input_path)
            output_file = os.path.join(output_dir, filename.replace('.txt', '_protein.fasta'))
            translate_dna(input_path, output_file)
            print(f"Translated {input_path} to {output_file}")

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python script.py <input_path> <output_dir>")
        sys.exit(1)
    
    input_path = sys.argv[1]
    output_dir = sys.argv[2]
    
    process_files(input_path, output_dir)

----------------------------------------------------
#to run above python script for translation

#!/bin/bash

#SBATCH --partition=defq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=50g
#SBATCH --time=10:00:00
#SBATCH --job-name=DNA_AA_translation
#SBATCH --output=/gpfs01/home/mbxss25/slurm-%x-%j.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=mbxss25@exmail.nottingham.ac.uk

input_dir="subject_db"
output_dir="aa_data"

python ./translate.py "$input_dir" "$output_dir"

#output file: 5_aa_data.zip
---------------------------------------------------------
#to merge all the translated sequnces in single fasta file.

#!/bin/bash

#SBATCH --partition=defq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=50g
#SBATCH --time=10:00:00
#SBATCH --job-name=merging_aa
#SBATCH --output=/gpfs01/home/mbxss25/slurm-%x-%j.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=mbxss25@exmail.nottingham.ac.uk

# Define the directory containing your FASTA files
directory="./aa_data"

# Define the output file where the merged FASTA will be saved
output_file="./merged_all_aminoacids"

# Ensure the output file is empty
> "$output_file"

# Loop through all FASTA files in the specified directory, sorted alphabetically
for file in $(find "$directory" -name '*.fasta' | sort); do
    echo "Merging $file into $output_file"
    # Optionally, you can add a newline before each new file's content to ensure no run-ons from previous files
    echo "" >> "$output_file"
    cat "$file" >> "$output_file"
done

echo "All FASTA files have been merged into $output_file"

#Output file: 6_merged_all_aminoacids.fasta.zip
---------------------------------------------------------
# For extracting the suquencesfrom the concated amino acid file, making a gene cluster.

#!/bin/bash

#SBATCH --partition=defq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=50g
#SBATCH --time=10:00:00
#SBATCH --job-name=cluster_formation
#SBATCH --output=/gpfs01/home/mbxss25/slurm-%x-%j.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=mbxss25@exmail.nottingham.ac.uk

# Define the input and output file paths
input_file="merged_all_aminoacids.fasta"                                                #file containg all the amino acid sequences
output_file="ampd3_cluster.fasta"                                                       #output fasta file of seperate cluster

# Use sed to extract lines containing "ampd3" and redirect the output to the new file
sed -n '/ampd3/p' "$input_file" > "$output_file"

echo "Successfully separated entries containing 'ampd3' into $output_file"

#Note: This code is just for single file, executed manually for each and every gene for 100% compliance.

#Output file: 7. merged_cluster.zip (This is directory where the output of above code is stored by the name of "ampd3_cluster", and similar genes utilized in study.)

---------------------------------------------------------
# Making database for the BLAST.

#!/bin/bash

#SBATCH --partition=defq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=50g
#SBATCH --time=10:00:00
#SBATCH --job-name=mkdatabase01
#SBATCH --output=/gpfs01/home/mbxss25/slurm-%x-%j.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=mbxss25@exmail.nottingham.ac.uk

# Directory where your gene cluster FASTA files are stored
CLUSTER_DIR="./merged_clusters"

# Directory to store the created BLAST databases
DB_DIR="./new_blastmkdata"

# Create the database directory if it does not exist
mkdir -p $DB_DIR

# Load BLAST module if necessary
module load blast-uoneasy/2.14.1-gompi-2023a                #for ada

# Loop through each FASTA file in the cluster directory
for fasta_file in $CLUSTER_DIR/*.fasta; do
    # Get the base name for the fasta file to name the database
    base_name=$(basename $fasta_file .fasta)

    # Create a BLAST database for each FASTA file
    makeblastdb -in $fasta_file -dbtype prot -out $DB_DIR/$base_name -title "$base_name DB"

    echo "Database created for $base_name at $DB_DIR/$base_name"
done

echo "All databases have been created."

#Output file: 8. new_blastmkdata.zip 

#The code above make database for the BLAST. this generates pdb, phr, pin, pjs, psq, ptf and pto files for each grnrcluster.

---------------------------------------------------------
# BLAST

#!/bin/bash

#SBATCH --partition=defq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=50g
#SBATCH --time=10:00:00
#SBATCH --job-name=final_blast
#SBATCH --output=/gpfs01/home/mbxss25/slurm-%x-%j.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=mbxss25@exmail.nottingham.ac.uk

# Load the BLAST module
module load blast-uoneasy/2.14.1-gompi-2023a

# Define directories
QUERY_DIR="./query_seqs"
DB_DIR="./new_blastmkdata"
OUT_DIR="./final_blast"

# Ensure output directory exists
mkdir -p ${OUT_DIR}

# Run BLAST for the specified query and database
blastp -query ${QUERY_DIR}/ascl2_extracted.fa -db ${DB_DIR}/ascl2_cluster -out ${OUT_DIR}/ascl2_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/casd1_extracted.fa -db ${DB_DIR}/casd1_cluster -out ${OUT_DIR}/casd1_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/cd81_extracted.fa -db ${DB_DIR}/cd81_cluster -out ${OUT_DIR}/cd81_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/cdh15_extracted.fa -db ${DB_DIR}/cdh15_cluster -out ${OUT_DIR}/cdh15_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/cdkn1c_extracted.fa -db ${DB_DIR}/cdkn1c_cluster -out ${OUT_DIR}/cdkn1c_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/commd1_extracted.fa -db ${DB_DIR}/commd1_cluster -out ${OUT_DIR}/commd1_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/dcn_extracted.fa -db ${DB_DIR}/dcn_cluster -out ${OUT_DIR}/dcn_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/dhcr7_extracted.fa -db ${DB_DIR}/dhcr7_cluster -out ${OUT_DIR}/dhcr7_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/dio3_extracted.fa -db ${DB_DIR}/dio3_cluster -out ${OUT_DIR}/dio3_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/dlk1_extracted.fa -db ${DB_DIR}/dlk1_cluster -out ${OUT_DIR}/dlk1_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/dlx5_extracted.fa -db ${DB_DIR}/dlx5_cluster -out ${OUT_DIR}/dlx5_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/dscam_extracted.fa -db ${DB_DIR}/dscam_cluster -out ${OUT_DIR}/dscam_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/gab1_extracted.fa -db ${DB_DIR}/gab1_cluster -out ${OUT_DIR}/gab1_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/gatm_extracted.fa -db ${DB_DIR}/gatm_cluster -out ${OUT_DIR}/gatm_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/glis3_extracted.fa -db ${DB_DIR}/glis3_cluster -out ${OUT_DIR}/glis3_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/grb10_extracted.fa -db ${DB_DIR}/grb10_cluster -out ${OUT_DIR}/grb10_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/h13_extracted.fa -db ${DB_DIR}/h13_cluster -out ${OUT_DIR}/h13_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/htr2a_extracted.fa -db ${DB_DIR}/htr2a_cluster -out ${OUT_DIR}/htr2a_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/igf2_extracted.fa -db ${DB_DIR}/igf2_cluster -out ${OUT_DIR}/igf2_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/igf2r_extracted.fa -db ${DB_DIR}/igf2r_cluster -out ${OUT_DIR}/igf2r_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/impact_extracted.fa -db ${DB_DIR}/impact_cluster -out ${OUT_DIR}/impact_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/lin28b_extracted.fa -db ${DB_DIR}/lin28b_cluster -out ${OUT_DIR}/lin28b_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/lrrtm1_extracted.fa -db ${DB_DIR}/lrrtm1_cluster -out ${OUT_DIR}/lrrtm1_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/naa60_extracted.fa -db ${DB_DIR}/naa60_cluster -out ${OUT_DIR}/naa60_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/osbpl5_extracted.fa -db ${DB_DIR}/osbpl5_cluster -out ${OUT_DIR}/osbpl5_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/plagl1_extracted.fa -db ${DB_DIR}/plagl1_cluster -out ${OUT_DIR}/plagl1_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/pon2_extracted.fa -db ${DB_DIR}/pon2_cluster -out ${OUT_DIR}/pon2_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/pon3_extracted.fa -db ${DB_DIR}/pon3_cluster -out ${OUT_DIR}/pon3_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/rb1_extracted.fa -db ${DB_DIR}/rb1_cluster -out ${OUT_DIR}/rb1_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/sfmbt2_extracted.fa -db ${DB_DIR}/sfmbt2_cluster -out ${OUT_DIR}/sfmbt2_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/slc38a4_extracted.fa -db ${DB_DIR}/slc38a4_cluster -out ${OUT_DIR}/slc38a4_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/tssc4_extracted.fa -db ${DB_DIR}/tssc4_cluster -out ${OUT_DIR}/tssc4_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/zc3h12c_extracted.fa -db ${DB_DIR}/zc3h12c_cluster -out ${OUT_DIR}/zc3h12c_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/zfat_extracted.fa -db ${DB_DIR}/zfat_cluster -out ${OUT_DIR}/zfat_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/zim2_extracted.fa -db ${DB_DIR}/zim2_cluster -out ${OUT_DIR}/zim2_blast_results.txt -evalue 1e-7 -outfmt 6
blastp -query ${QUERY_DIR}/znf597_extracted.fa -db ${DB_DIR}/znf597_cluster -out ${OUT_DIR}/znf597_blast_results.txt -evalue 1e-7 -outfmt 6

echo "BLAST search completed for ampd3. Results are saved in ${OUT_DIR}/ampd3_blast_results.txt"

# outputfile: 9.final_blast.zip

# Note: The parameter of E value was programmed as 1e-7 for the precised homologoues sequences, and outformat was set as 6.

------------------------------------------------------------------------------------------------------------------------------------------------------------------
# To extract the homologoues sequences which show hits in BLAST.

import os
from Bio import SeqIO
import pandas as pd

def extract_sequences(subject_dir, blast_results_dir, output_dir):
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    # Iterate through each BLAST result file in the directory
    for blast_file in os.listdir(blast_results_dir):
        if blast_file.endswith('.txt'):
            gene_cluster = blast_file.replace('_blast_results_filtered.txt', '')  # Adjust the replacement to match the actual file naming
            fasta_file = os.path.join(subject_dir, f"{gene_cluster}_cluster.fasta")
            output_fasta = os.path.join(output_dir, f"{gene_cluster}_final.fasta")
            
            # Check if the FASTA file exists before proceeding
            if not os.path.exists(fasta_file):
                print(f"No FASTA file found for {fasta_file}")
                continue
            
            # Read the BLAST results to get the list of sequence IDs
            blast_df = pd.read_csv(os.path.join(blast_results_dir, blast_file), sep='\t', header=None, usecols=[1])
            blast_df.columns = ['sseqid']
            accession_numbers = set(blast_df['sseqid'])
            
            # Load the sequences from the FASTA file
            sequences_to_write = []
            for seq in SeqIO.parse(fasta_file, 'fasta'):
                if seq.id in accession_numbers:
                    sequences_to_write.append(seq)
                
            # Write the filtered sequences to the output FASTA file
            SeqIO.write(sequences_to_write, output_fasta, 'fasta')

# Define the directories
subject_dir = './merged_clusters'                #seperate gene cluster containing sequences for each species
blast_results_dir = './final_blast'              #hits of the homologoues sequences
output_dir = './ready_clusters'                  #new directory to store Homologoues sequences. 

# Extract sequences
extract_sequences(subject_dir, blast_results_dir, output_dir)

#the above python script was saved by the name of cluster_ready.py

#to execute the above script below sbatch job is submitted.

#!/bin/bash

#SBATCH --partition=defq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=50g
#SBATCH --time=10:00:00
#SBATCH --job-name=homologues
#SBATCH --output=/gpfs01/home/mbxss25/slurm-%x-%j.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=mbxss25@exmail.nottingham.ac.uk

python cluster_ready.py

echo "done prcoessing"

#output file: 10. extracted sequences after blast complete_gene_cluster.fasta
